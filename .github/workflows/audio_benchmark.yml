name: Audio Benchmark Regression Testing

on:
  pull_request:
    paths:
      - 'v2/src/audio/**'
      - 'v2/test/test_audio/**'
      - 'v2/test/baseline/**'
      - 'v2/platformio.ini'
      - '.github/workflows/audio_benchmark.yml'
  push:
    branches:
      - main
      - 'feature/audio-*'
    paths:
      - 'v2/src/audio/**'
      - 'v2/test/test_audio/**'
  workflow_dispatch:  # Allow manual trigger

env:
  PLATFORMIO_CORE_DIR: .platformio

jobs:
  audio-benchmark:
    name: Audio Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better context

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Cache PlatformIO
        uses: actions/cache@v4
        with:
          path: |
            ~/.platformio
            .platformio
            v2/.pio
          key: ${{ runner.os }}-pio-${{ hashFiles('**/platformio.ini') }}
          restore-keys: |
            ${{ runner.os }}-pio-

      - name: Install PlatformIO
        run: |
          python -m pip install --upgrade pip
          pip install platformio
          pio --version

      - name: Install analysis tools
        run: |
          pip install -r v2/test/tools/requirements.txt || echo "No requirements.txt, skipping"

      - name: Build native test environment
        working-directory: v2
        run: |
          pio test -e native_test --without-testing --verbose

      - name: Run audio benchmark tests
        id: benchmark
        working-directory: v2
        continue-on-error: true
        run: |
          echo "Running audio benchmark tests..."
          pio test -e native_test -f test_pipeline_benchmark --verbose > benchmark_output.log 2>&1 || true
          cat benchmark_output.log

      - name: Parse benchmark results
        id: parse
        run: |
          python v2/test/tools/parse_benchmark_serial.py \
            v2/benchmark_output.log \
            --output v2/test/results/current_benchmark.json \
            --platformio

          # Also generate summary for console
          python v2/test/tools/parse_benchmark_serial.py \
            v2/benchmark_output.log \
            --format summary \
            --platformio

      - name: Create results directory
        run: mkdir -p v2/test/results

      - name: Detect regressions
        id: regression
        continue-on-error: true
        run: |
          # Compare against baseline
          python v2/test/tools/detect_regressions.py \
            v2/test/baseline/benchmark_baseline.json \
            v2/test/results/current_benchmark.json \
            --format console

          # Generate markdown report for PR comment
          python v2/test/tools/detect_regressions.py \
            v2/test/baseline/benchmark_baseline.json \
            v2/test/results/current_benchmark.json \
            --format markdown \
            --output v2/test/results/regression_report.md

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: |
            v2/test/results/*.json
            v2/test/results/*.md
            v2/benchmark_output.log
          retention-days: 30

      - name: Comment on PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const reportPath = 'v2/test/results/regression_report.md';

            let comment = '## Audio Benchmark Results\n\n';

            try {
              if (fs.existsSync(reportPath)) {
                comment = fs.readFileSync(reportPath, 'utf8');
              } else {
                comment += ':x: **Error**: Benchmark results could not be generated.\n\n';
                comment += 'Check the workflow logs for details.';
              }
            } catch (error) {
              comment += `:warning: **Warning**: Could not read results: ${error.message}\n\n`;
              comment += 'Check the workflow logs for details.';
            }

            // Add workflow run link
            comment += '\n\n---\n';
            comment += `[View full workflow run](${context.payload.repository.html_url}/actions/runs/${context.runId})`;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Audio Benchmark Results')
            );

            // Create or update comment
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Check for regression failures
        if: always()
        run: |
          # Re-run regression detection with exit code check
          python v2/test/tools/detect_regressions.py \
            v2/test/baseline/benchmark_baseline.json \
            v2/test/results/current_benchmark.json \
            --format console

      - name: Generate job summary
        if: always()
        run: |
          echo "## Audio Benchmark Job Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f v2/test/results/regression_report.md ]; then
            cat v2/test/results/regression_report.md >> $GITHUB_STEP_SUMMARY
          else
            echo ":x: Benchmark results not available" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Output" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -n 50 v2/benchmark_output.log >> $GITHUB_STEP_SUMMARY || echo "No output log available"
          echo '```' >> $GITHUB_STEP_SUMMARY

  update-baseline:
    name: Update Baseline (Manual)
    runs-on: ubuntu-latest
    needs: audio-benchmark
    if: github.event_name == 'workflow_dispatch' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: v2/test/results

      - name: Update baseline
        run: |
          # Copy current results to baseline
          cp v2/test/results/current_benchmark.json v2/test/baseline/benchmark_baseline.json

          # Update metadata
          python -c "
          import json
          from datetime import datetime

          with open('v2/test/baseline/benchmark_baseline.json', 'r') as f:
              data = json.load(f)

          if '_metadata' not in data:
              data['_metadata'] = {}

          data['_metadata']['updated'] = datetime.now().isoformat()
          data['_metadata']['git_commit'] = '${{ github.sha }}'
          data['_metadata']['updated_by'] = 'GitHub Actions'

          with open('v2/test/baseline/benchmark_baseline.json', 'w') as f:
              json.dump(data, f, indent=2)
          "

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: 'chore(test): update audio benchmark baseline'
          title: 'Update Audio Benchmark Baseline'
          body: |
            ## Automated Baseline Update

            This PR updates the audio benchmark baseline metrics based on the latest test run.

            **Source**: ${{ github.sha }}
            **Workflow**: ${{ github.run_id }}

            ### Review Checklist
            - [ ] Verify metrics are improvement, not regression
            - [ ] Check test conditions match baseline requirements
            - [ ] Confirm changes are intentional optimizations

            **Warning**: Only merge if these results represent genuine improvements!
          branch: chore/update-benchmark-baseline
          delete-branch: true
